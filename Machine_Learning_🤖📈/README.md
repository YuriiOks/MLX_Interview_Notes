# Machine Learning Notebooks

This folder contains Jupyter notebooks and diagrams covering various machine learning topics organized by rounds.

## Round 1: Fundamentals of Machine Learning

| Topic                          | Question (and link)                                                                                                                        | Brief Explanation                                                                     |
|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| Classification vs. Regression| [Explain the key differences between classification and regression tasks in machine learning.](./Round1/01_classification_vs_regression.ipynb)  | Discusses how classification predicts discrete labels while regression forecasts continuous values. |
| Feature in ML                  | [Define a feature in machine learning and explain its significance.](./Round1/02_feature_in_ml.ipynb)                                        | Highlights the importance of features in driving model performance.                   |
| Overfitting                    | [What is overfitting in machine learning and what methods can prevent it?](./Round1/03_overfitting.ipynb)                                      | Describes overfitting, its risks, and approaches to prevent it.                         |
| Validation Set                 | [What is the purpose of a validation set in model training?](./Round1/04_validation_set.ipynb)                                               | Explores the role of the validation set in tuning and evaluating models.                |
| Cross Validation               | [How does cross-validation enhance model generalization?](./Round1/05_cross_validation.ipynb)                                                | Explains how cross-validation helps ensure robust model performance by preventing overfitting. |
| Hyperparameters                | [What are hyperparameters in machine learning, and how can they be effectively tuned?](./Round1/06_hyperparameters.ipynb)                        | Introduces hyperparameters and common tuning methods for model optimization.            |
| Confusion Matrix               | [How do you interpret a confusion matrix for classification tasks?](./Round1/07_confusion_matrix.ipynb)                                      | Details the components of a confusion matrix for evaluating classifier performance.      |
| Decision Trees                 | [Describe how decision trees work and discuss their potential limitations.](./Round1/08_decision_tree.ipynb)                                   | Outlines the structure and functioning of decision trees in predictive modeling.         |
| Regularisation                 | [What is regularisation in machine learning, and when should it be applied?](./Round1/09_regularisation.ipynb)                                  | Covers techniques such as L1 and L2 regularisation to reduce overfitting and improve generalization. |
| Activation Functions           | [Why are activation functions essential in neural networks, and what are some common examples?](./Round1/10_activation_functions.ipynb)         | Explains the role of activation functions in introducing non-linearity into models.        |

## Round 2: Advanced Topics and Neural Network Flows

| Topic                                  | Question (and link)                                                                                                                                | Brief Explanation                                                                              |
|----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| Handling Imbalanced Datasets           | [How do you handle imbalanced datasets in classification problems?](./Round2/01_imbalanced_datasets.ipynb)                                         | Discusses techniques to balance classes using oversampling, undersampling, or class weighting.    |
| Bagging vs. Boosting                   | [Can you explain the difference between bagging and boosting?](./Round2/02_bagging_vs_boosting.ipynb)                                               | Compares ensemble methods that reduce variance (bagging) versus those that reduce bias (boosting).  |
| Transfer Learning                      | [What is transfer learning, and when would you use it?](./Round2/03_transfer_learning.ipynb)                                                         | Describes reusing pre-trained models to accelerate learning on new tasks.                         |
| Backpropagation                        | [How does the backpropagation algorithm work in training neural networks?](./Round2/04_backpropagation.ipynb)                                        | Explains how error signals propagate backward to adjust weights during training.                  |
| GANs                                   | [What are Generative Adversarial Networks (GANs), and how do they work?](./Round2/05_gans.ipynb)                                                     | Introduces a framework where two networks contest to generate realistic data samples.             |
| Vanishing Gradient                     | [What is the vanishing gradient problem, and how can it be mitigated?](./Round2/06_vanishing_gradient.ipynb)                                         | Highlights how shrinking gradients hinder learning and approaches to counteract this issue.         |
| L1 vs. L2 Regularisation               | [Can you explain the difference between L1 and L2 regularisation?](./Round2/07_l1_vs_l2_regularisation.ipynb)                                        | Discusses how L1 promotes sparsity while L2 smooths weight updates to prevent overfitting.          |
| Support Vector Machines                | [What are support vector machines (SVMs), and how do they find the optimal hyperplane?](./Round2/08_svms_optimal_hyperplane.ipynb)                  | Describes SVMs' method for maximizing the margin between classes using a decision boundary.         |
| Recurrent Neural Networks              | [What is a recurrent neural network (RNN), and how does it differ from a standard neural network?](./Round2/09_rnn_vs_standard_nn.ipynb)            | Explains how RNNs process sequential data by maintaining an internal state over time.              |
| Dropout                                | [How does dropout work in neural networks, and why is it effective?](./Round2/10_dropout.ipynb)                                                      | Introduces dropout as a regularisation strategy to prevent overfitting by randomly disabling neurons. |
| Kernel in SVMs                         | [What is the purpose of a kernel in SVMs, and how does it transform the data?](./Round2/11_kernel_in_svms.ipynb)                                   | Explains how kernel functions enable SVMs to perform non-linear classification efficiently.         |
| Attention Mechanism                    | [How does the attention mechanism work in models like Transformers?](./Round2/12_attention_mechanism.ipynb)                                       | Details how attention allows models to focus on key parts of the input for improved performance.   |
| Gradient Optimizers                    | [What are the trade-offs between different types of gradient-based optimization algorithms like SGD, Adam, and RMSprop?](./Round2/13_gradient_optimizers_tradeoffs.ipynb) | Reviews benefits and drawbacks of popular optimizers in training neural networks.                  |
| Autoencoders                           | [How do autoencoders work, and what are their applications in machine learning?](./Round2/14_autoencoders.ipynb)                                    | Describes unsupervised architectures that learn efficient data encodings through reconstruction.    |
| Curse of Dimensionality                | [Can you explain the concept of 'curse of dimensionality' in the context of machine learning?](./Round2/15_curse_of_dimensionality.ipynb)         | Discusses the challenges and computational burdens associated with high-dimensional data.          |
| Bagging vs. Stacking                   | [What is the difference between bagging and stacking in ensemble methods?](./Round2/16_bagging_vs_stacking.ipynb)                                  | Compares simple ensemble methods like bagging with more complex stacking techniques.               |
| Custom Loss Function                   | [How would you implement a custom loss function in a neural network?](./Round2/17_custom_loss_function.ipynb)                                       | Explains the process for designing and integrating task-specific loss functions.                   |
| Adversarial Examples                   | [What is an adversarial example, and how can machine learning models be made robust against them?](./Round2/18_adversarial_examples.ipynb)       | Discusses how intentionally perturbed inputs can mislead models and ways to improve model defense.   |
| Conv vs. Fully Connected Layers        | [Can you explain the difference between a convolutional layer and a fully connected layer in a neural network?](./Round2/19_conv_vs_fullyconnected.ipynb) | Details the architectural differences and applications of convolutional versus dense layers.       |
| Cross-Entropy Loss                     | [Explain the concept of cross-entropy loss in classification problems and how it is used in training neural networks.](./Round2/20_cross_entropy_loss.ipynb)   | Describes how cross-entropy measures the difference between predicted and true probability distributions. |
