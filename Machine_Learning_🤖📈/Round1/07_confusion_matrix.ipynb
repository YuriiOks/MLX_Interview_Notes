{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7Ô∏è‚É£ Confusion Matrix: Understanding Model Performance üìäü§ñ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üí° Real-Life Analogy: VAR (Video Assistant Referee) in Football ‚öΩ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a **VAR system** is used to review whether a goal was **offside or not**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **True Positive (TP) ‚úÖ** ‚Üí The referee **correctly** calls it offside.  \n",
    "- **False Positive (FP) ‚ùå** ‚Üí The referee **incorrectly** calls offside when it wasn't.  \n",
    "- **True Negative (TN) ‚úÖ** ‚Üí The referee **correctly** allows a goal (not offside).  \n",
    "- **False Negative (FN) ‚ùå** ‚Üí The referee **misses an offside** and allows an invalid goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **A confusion matrix tells us how well a model classifies different categories!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå What is a Confusion Matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ A **confusion matrix** is a table that shows the **performance of a classification model** by comparing its predictions with actual values.  \n",
    "‚úÖ It helps in evaluating errors and understanding whether the model is **making more false positives or false negatives**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **General Structure of a Confusion Matrix:**\n",
    "\n",
    "| **Actual \\ Predicted** | **Positive (1)** | **Negative (0)** |  \n",
    "|----------------------|---------------|---------------|  \n",
    "| **Positive (1)**     | **True Positive (TP)** ‚úÖ | **False Negative (FN)** ‚ùå |  \n",
    "| **Negative (0)**     | **False Positive (FP)** ‚ùå | **True Negative (TN)** ‚úÖ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positives)** ‚Üí Model correctly predicts **Positive**.  \n",
    "- **TN (True Negatives)** ‚Üí Model correctly predicts **Negative**.  \n",
    "- **FP (False Positives, Type I Error)** ‚Üí Model **wrongly** predicts Positive.  \n",
    "- **FN (False Negatives, Type II Error)** ‚Üí Model **wrongly** predicts Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **A perfect model has only True Positives & True Negatives, with 0 False Positives & False Negatives!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìä Example: Confusion Matrix in Football (Predicting Goals in a Match) ‚öΩ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Scenario:** A machine learning model predicts **whether a player will score in a match**.  \n",
    "- **Positive (1) = The player scores a goal.**  \n",
    "- **Negative (0) = The player does not score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Actual vs. Predicted Outcomes:**\n",
    "\n",
    "| **Actual \\ Predicted** | **Predicted: Goal (1)** | **Predicted: No Goal (0)** |  \n",
    "|----------------------|-----------------|-----------------|  \n",
    "| **Actual: Goal (1)** | **TP = 50** ‚úÖ  | **FN = 10** ‚ùå  \n",
    "| **Actual: No Goal (0)** | **FP = 15** ‚ùå  | **TN = 25** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Interpretation:**  \n",
    "- **TP = 50** ‚Üí Model correctly predicted **50 goals**.  \n",
    "- **FP = 15** ‚Üí Model predicted a goal **when there was none** (false alarm).  \n",
    "- **FN = 10** ‚Üí Model **missed 10 actual goals** (false negatives).  \n",
    "- **TN = 25** ‚Üí Model correctly predicted **25 non-goals**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Key Insight:**  \n",
    "- If **False Negatives (FN) are high**, the model **fails to detect goal scorers**.  \n",
    "- If **False Positives (FP) are high**, the model **incorrectly predicts too many goals**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìä Example: Confusion Matrix in NBA (Predicting All-Star Selections) üèÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Scenario:** A model predicts whether an NBA player will become an **All-Star**.  \n",
    "- **Positive (1) = Selected as an All-Star.**  \n",
    "- **Negative (0) = Not an All-Star.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Confusion Matrix Output:**\n",
    "\n",
    "| **Actual \\ Predicted** | **Predicted: All-Star (1)** | **Predicted: Not All-Star (0)** |  \n",
    "|----------------------|-----------------|-----------------|  \n",
    "| **Actual: All-Star (1)** | **TP = 30** ‚úÖ  | **FN = 5** ‚ùå  \n",
    "| **Actual: Not All-Star (0)** | **FP = 20** ‚ùå  | **TN = 45** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Interpretation:**  \n",
    "- **TP = 30** ‚Üí Model correctly predicted **30 All-Stars**.  \n",
    "- **FP = 20** ‚Üí Model incorrectly predicted **20 non-All-Stars as All-Stars**.  \n",
    "- **FN = 5** ‚Üí Model **missed 5 actual All-Stars**.  \n",
    "- **TN = 45** ‚Üí Model correctly predicted **45 non-All-Stars**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Key Takeaways:**  \n",
    "- If **False Negatives (FN) are high**, **deserving players are missed** (bad for scouting).  \n",
    "- If **False Positives (FP) are high**, **undeserving players are selected**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üÜö Classification Metrics from the Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TP, FP, FN, and TN, we calculate key evaluation metrics:\n",
    "\n",
    "| Metric                            | Formula | Interpretation |\n",
    "|-----------------------------------|---------|----------------|\n",
    "| **Accuracy**                      | $\\frac{TP + TN}{TP + TN + FP + FN}$ | Overall correctness of the model. |\n",
    "| **Precision (Positive Predictive Value)** | $\\frac{TP}{TP + FP}$ | Of all predicted positives, how many were correct? |\n",
    "| **Recall (Sensitivity, True Positive Rate)** | $\\frac{TP}{TP + FN}$ | Of all actual positives, how many were correctly identified? |\n",
    "| **F1-Score**                      | $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Harmonic mean of Precision & Recall (Best for imbalanced data). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Choosing the Right Metric:**  \n",
    "- **High Precision Needed?** ‚Üí Use **Precision** (e.g., Fraud Detection üí≥, Medical Diagnosis üè•).  \n",
    "- **High Recall Needed?** ‚Üí Use **Recall** (e.g., Cancer Detection üè•, Goal Scoring Models ‚öΩ).  \n",
    "- **Balanced?** ‚Üí Use **F1-Score** (e.g., Sports Predictions üèÄ‚öΩ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üõ†Ô∏è Python Code: Confusion Matrix & Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[5 2]\n",
      " [2 6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71         7\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.73      0.73      0.73        15\n",
      "weighted avg       0.73      0.73      0.73        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Actual vs. Predicted Labels (Football Goal Prediction Example)\n",
    "y_actual = np.array([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1])\n",
    "y_predicted = np.array([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_actual, y_predicted)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Compute Classification Report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Key Insights:**  \n",
    "- **Precision:** 71% of **Class 0** predictions and 75% of **Class 1** predictions were correct.\n",
    "- **Recall:** 71% of **actual Class 0** and 75% of **actual Class 1** were correctly identified.\n",
    "- **F1-Score:** The harmonic mean of Precision and Recall is 0.71 for Class 0 and 0.75 for Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Output Example:**  \n",
    "```\n",
    "Confusion Matrix:\n",
    "[[4 2]  # TN = 4, FP = 2\n",
    " [1 8]] # FN = 1, TP = 8\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score\n",
    "           0       0.80     0.67      0.73\n",
    "           1       0.80     0.89      0.84\n",
    "```\n",
    "- Instead of **one accuracy score**, we get detailed metrics for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üöÄ Applications of the Confusion Matrix in AI/ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Football Scouting (Predicting Goal Scorers) ‚öΩ** ‚Üí Ensures the model correctly identifies **top-performing players**.  \n",
    "‚úÖ **NBA Analytics (All-Star Predictions) üèÄ** ‚Üí Reduces **false positives (overrated players)**.  \n",
    "‚úÖ **Medical Diagnosis (Cancer Detection) üè•** ‚Üí Balances **false positives (unnecessary tests) & false negatives (missed cancers)**.  \n",
    "‚úÖ **Spam Detection üìß** ‚Üí Improves accuracy in detecting **real vs. spam emails**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üî• Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ **A confusion matrix shows how well a classification model predicts outcomes.**  \n",
    "2Ô∏è‚É£ **It contains True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN).**  \n",
    "3Ô∏è‚É£ **Precision, Recall, and F1-score help assess model performance.**  \n",
    "4Ô∏è‚É£ **Used in sports analytics, medical diagnosis, fraud detection, and spam filtering.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
