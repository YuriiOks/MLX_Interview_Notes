{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2Ô∏è‚É£ Bagging vs. Boosting: Key Differences & Applications ü§ñ‚ö°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üí° Real-Life Analogy: Coaching Strategies in Football & NBA ‚öΩüèÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're coaching a football or NBA team:\n",
    "\n",
    "- **Bagging üèÜ** ‚Üí Each assistant coach **independently trains different strategies**, and then you combine their insights.  \n",
    "- **Boosting üî•** ‚Üí You **focus on fixing past mistakes**, gradually improving weak areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Bagging reduces variance (avoids overfitting), while Boosting reduces bias (improves weak predictions).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìå What is Bagging? (Bootstrap Aggregating)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Bagging = Training multiple independent models on different random subsets of data and averaging their predictions.**  \n",
    "‚úÖ **Purpose:** Reduces variance (prevents overfitting) by averaging multiple weak models.  \n",
    "‚úÖ **Best for:** High-variance models like Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **How Bagging Works (Steps):**  \n",
    "1Ô∏è‚É£ Create **random subsets** of the training data (sampling with replacement).  \n",
    "2Ô∏è‚É£ Train **separate models** (usually Decision Trees) on each subset.  \n",
    "3Ô∏è‚É£ Aggregate predictions (majority vote for classification, average for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Popular Bagging Algorithms:**  \n",
    "- **Random Forest üå≥** ‚Üí Uses multiple Decision Trees to improve accuracy.  \n",
    "- **Bagged Decision Trees** ‚Üí Similar to Random Forest but without feature randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìå What is Boosting? (Sequential Learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Boosting = Training models sequentially, with each new model focusing on correcting the mistakes of the previous one.**  \n",
    "‚úÖ **Purpose:** Reduces bias (improves weak learners by focusing on hard-to-classify cases).  \n",
    "‚úÖ **Best for:** High-bias models like Logistic Regression, Shallow Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **How Boosting Works (Steps):**  \n",
    "1Ô∏è‚É£ Train an initial weak model.  \n",
    "2Ô∏è‚É£ Identify misclassified data points and assign **higher weights** to them.  \n",
    "3Ô∏è‚É£ Train a new model that focuses on fixing previous mistakes.  \n",
    "4Ô∏è‚É£ Repeat until **errors are minimized**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Popular Boosting Algorithms:**  \n",
    "- **AdaBoost (Adaptive Boosting) üèÜ** ‚Üí Adjusts sample weights to focus on misclassified cases.  \n",
    "- **Gradient Boosting Machines (GBM) üìà** ‚Üí Learns from residual errors.  \n",
    "- **XGBoost (Extreme Gradient Boosting) üî•** ‚Üí Fast, optimized version of GBM (used in Kaggle competitions).  \n",
    "- **LightGBM üåü** ‚Üí Faster than XGBoost for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìä Key Differences: Bagging vs. Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                      | **Bagging (Random Forest) üèÜ**                  | **Boosting (XGBoost) üî•**                         |\n",
    "|------------------------------|-----------------------------------------------|---------------------------------------------------|\n",
    "| **Goal**                     | Reduce **variance** (overfitting).             | Reduce **bias** (improve weak models).            |\n",
    "| **How It Works?**            | Trains models in **parallel** on random subsets.| Trains models **sequentially**, fixing previous mistakes. |\n",
    "| **Effect on Overfitting?**   | Prevents overfitting ‚úÖ                         | Can overfit if not tuned ‚ùå                       |\n",
    "| **Final Prediction?**        | **Averaging** (regression) or majority vote (classification). | **Weighted sum** of all models' outputs.         |\n",
    "| **Best For?**                | **High-variance models** (Decision Trees).      | **Weak models needing improvement** (Shallow Trees, Logistic Regression). |\n",
    "| **Speed?**                   | Faster (parallel training). ‚ö°                  | Slower (sequential learning). ‚è≥                  |\n",
    "| **Example Algorithm**        | Random Forest üå≥                               | XGBoost üî•                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Use Bagging when overfitting is a problem.**  \n",
    "‚úÖ **Use Boosting when underfitting (high bias) is an issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìä Example 1: Predicting Football Wins Using Bagging (Random Forest) ‚öΩ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Scenario:** You predict whether a football team **will win or lose** based on:\n",
    "- **Shots on Target üéØ**  \n",
    "- **Possession % ‚öΩ**  \n",
    "- **Pass Accuracy % üèÜ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Bagging Approach (Random Forest)**  \n",
    "‚úÖ **Steps:**  \n",
    "1Ô∏è‚É£ Train **multiple Decision Trees** on different random samples of matches.  \n",
    "2Ô∏è‚É£ Each tree makes **independent predictions** (win or lose).  \n",
    "3Ô∏è‚É£ The final prediction is the **majority vote**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Python Implementation (Random Forest - Bagging)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(0)\n",
    "n_samples = 200\n",
    "\n",
    "# Features:\n",
    "# Shots on Target: integers in [0, 15]\n",
    "shots_on_target = np.random.randint(0, 16, n_samples)\n",
    "\n",
    "# Possession %: floats in [30, 70]\n",
    "possession = np.random.uniform(30, 70, n_samples)\n",
    "\n",
    "# Pass Accuracy %: floats in [50, 100]\n",
    "pass_accuracy = np.random.uniform(50, 100, n_samples)\n",
    "\n",
    "# Combine features into a dataframe or numpy array\n",
    "X = np.column_stack((shots_on_target, possession, pass_accuracy))\n",
    "\n",
    "# Simulate target: win (1) or lose (0). \n",
    "# For simplicity, assume that higher possession and pass accuracy lead to wins.\n",
    "y = ((possession + pass_accuracy) > 120).astype(int)\n",
    "\n",
    "# Split data into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier (bagging approach)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Why Use Bagging?**  \n",
    "- Random Forest **reduces overfitting** in high-variance data like match outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìä Example 2: Predicting NBA MVP Using Boosting (XGBoost) üèÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Scenario:** You predict **NBA MVP winners** based on:\n",
    "- **Points Per Game (PPG) üèÄ**  \n",
    "- **Assists & Rebounds üìä**  \n",
    "- **Team Wins üèÜ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Boosting Approach (XGBoost)**  \n",
    "‚úÖ **Steps:**  \n",
    "1Ô∏è‚É£ Train an initial weak model (Shallow Decision Tree).  \n",
    "2Ô∏è‚É£ Identify **incorrect predictions** and increase their weight.  \n",
    "3Ô∏è‚É£ Train the next model to **fix previous mistakes**.  \n",
    "4Ô∏è‚É£ Repeat until **errors are minimized**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Python Implementation (XGBoost - Boosting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Features:\n",
    "# Points Per Game (PPG): floats in range [10, 35]\n",
    "ppg = np.random.uniform(10, 35, n_samples)\n",
    "\n",
    "# Combined metric for Assists & Rebounds: floats in range [5, 20]\n",
    "assists_rebounds = np.random.uniform(5, 20, n_samples)\n",
    "\n",
    "# Team Wins: integers in range [20, 82] (NBA regular season wins)\n",
    "team_wins = np.random.randint(20, 83, n_samples)\n",
    "\n",
    "# Combine features into a feature matrix\n",
    "X = np.column_stack((ppg, assists_rebounds, team_wins))\n",
    "\n",
    "# Simulate target: MVP (1) vs. Not MVP (0)\n",
    "# For simplicity, assume that higher values in ppg, assists_rebounds, and team wins contribute to MVP selection.\n",
    "# A simple scoring rule:\n",
    "score = ppg + assists_rebounds * 0.8 + (team_wins / 82) * 10  # normalization for team wins\n",
    "threshold = np.median(score)\n",
    "y = (score > threshold).astype(int)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize XGBoost Classifier representing our boosting approach\n",
    "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, eval_metric=\"logloss\", random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Why Use Boosting?**  \n",
    "- XGBoost **improves weak models by focusing on hard-to-classify MVP candidates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üöÄ When to Use Bagging vs. Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Scenario**                         | **Use Bagging (Random Forest) ‚úÖ**             | **Use Boosting (XGBoost) üî•**                   |\n",
    "|--------------------------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| **Predicting Football Wins** ‚öΩ          | Works well for random event-based games.       | If we need fine-tuned accuracy.               |\n",
    "| **Player Performance (NBA, EPL)** üèÄ     | If data has **high variance (streaky players)**. | If we need **detailed adjustments** (fixing weak models). |\n",
    "| **Medical Diagnosis** üè•               | When we need **generalization**.                 | When we need **high recall (avoiding false negatives).** |\n",
    "| **Stock Market Prediction** üìà         | Works well for **general trends**.               | If the goal is to **improve poor predictions**.  |\n",
    "| **Fraud Detection** üí≥                | Not ideal (random sampling might miss fraud cases). | Best choice (focuses on rare fraud events).     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Choose Bagging when the model overfits or has high variance.**  \n",
    "‚úÖ **Choose Boosting when the model underfits and needs improvement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üî• Final Takeaways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ **Bagging trains models in parallel (Random Forest), while Boosting trains models sequentially (XGBoost).**  \n",
    "2Ô∏è‚É£ **Bagging reduces variance (prevents overfitting), Boosting reduces bias (improves weak models).**  \n",
    "3Ô∏è‚É£ **Random Forest (Bagging) works well for high-variance datasets like sports analytics.**  \n",
    "4Ô∏è‚É£ **XGBoost (Boosting) works well for high-bias datasets like fraud detection & medical AI.**  \n",
    "5Ô∏è‚É£ **Use Bagging when we need stability, Boosting when we need fine-tuned accuracy.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
