# Mathematics Notebooks

This folder contains Jupyter notebooks that cover fundamental and advanced mathematical concepts essential for understanding AI, machine learning, and quantitative analysis, organized by rounds.

## Round 1: Fundamental Mathematical Concepts

| Topic                                  | Question (and link)                                                                                                          | Brief Explanation                                                                     |
|----------------------------------------|------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| Linear Regression                      | [Explain the concept of linear regression.](./Round1/01_linear_regression.ipynb)                                             | Explains the idea behind linear regression using least squares estimation.             |
| Derivatives                            | [What is the derivative of a function?](./Round1/02_derivatives.ipynb)                                                       | Introduces the derivative as the rate of change of a function.                        |
| Polynomial Functions                   | [What is a polynomial function? Give an example.](./Round1/03_polynomial_function.ipynb)                                      | Describes polynomial functions and provides real-world examples.                      |
| Linear Functions                       | [What is a linear function?](./Round1/04_linear_function.ipynb)                                                              | Defines linear functions and explains their constant rate of change.                   |
| Gradient of a Function                 | [What is the gradient of a function, and how is it used in optimization?](./Round1/05_gradient_of_function.ipynb)              | Explains the gradient and its role in optimization algorithms like gradient descent.   |
| Matrix Multiplication                  | [Explain how matrix multiplication works.](./Round1/06_matrix_multiplication.ipynb)                                          | Demonstrates how to perform matrix multiplication and its applications.                |
| Chain Rule                             | [What is the chain rule in calculus, and how is it applied when differentiating composite functions?](./Round1/07_chain_rule.ipynb) | Explains the chain rule and its application in differentiating composite functions.    |
| Law of Large Numbers                   | [What is the law of large numbers in probability theory?](./Round1/08_law_of_large_numbers.ipynb)                             | Outlines how increasing sample size leads to the convergence of the sample mean.         |
| Discrete vs. Continuous Probability    | [What is the difference between discrete and continuous probability distributions?](./Round1/09_discrete_vs_continuous_probability.ipynb) | Compares and contrasts discrete and continuous probability distributions.             |
| Euclidean Distance                     | [How do we compute the Euclidean distance between two points?](./Round1/10_euclidean_distance.ipynb)                           | Describes how to calculate Euclidean distance and its applications in clustering.       |

## Round 2: Advanced Mathematical Topics

| Topic                                      | Question (and link)                                                                                                                           | Brief Explanation                                                                                                                         |
|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| Eigenvalues & Eigenvectors                 | [Define eigenvalues and eigenvectors, and explain their importance in linear transformations.](./Round2/01_eigenvalues_eigenvectors.ipynb)    | Explores how these concepts capture key characteristics of linear transformations.                                                      |
| Dot Product                                | [What is a dot product of two vectors, and what does it represent geometrically?](./Round2/02_dot_product.ipynb)                                | Demonstrates how the dot product is computed and its geometric interpretation.                                                           |
| Bayes' Theorem                             | [What is Bayes' theorem, and how is it applied in probability theory?](./Round2/03_bayes_theorem.ipynb)                                        | Explains Bayes' theorem as a method for updating probabilities based on new evidence.                                                    |
| Gradient in Optimization                   | [Explain how gradient is used in optimization problems.](./Round2/04_gradient_in_optimization.ipynb)                                            | Describes how gradients guide iterative improvement in optimization methods.                                                           |
| Entropy and Information Loss               | [Explain the concept of entropy in information theory and its role in machine learning models.](./Round2/05_entropy_and_information_loss.ipynb)  | Explores how entropy quantifies uncertainty and influences decision-making in learning models.                                             |
| Central Limit Theorem                      | [What is the Central Limit Theorem, and why is it important in statistics?](./Round2/06_central_limit_theorem.ipynb)                             | Outlines the significance of the Central Limit Theorem in statistical inference and its impact on data analysis.                           |
| Logistic Regression Coefficients           | [How do you interpret the coefficients in a logistic regression model?](./Round2/07_logistic_regression_coefficients.ipynb)                      | Provides insights on understanding logistic regression weights and their statistical significance.                                       |
| Softmax Function                           | [When talking about the softmax function, why do we use an exponential and why do we divide by the sum of the exponentials?](./Round2/08_softmax_exponential.ipynb) | Explains the rationale behind the softmax activation, which normalizes outputs into a probability distribution.                           |
| Convex vs. Non-Convex Functions            | [What is the difference between a convex and a non-convex function, and why is this distinction important in optimization problems?](./Round2/09_convex_vs_nonconvex.ipynb) | Discusses why convexity matters for ensuring convergence and stability in optimization algorithms.                                       |
| Partial Derivative in Multivariable        | [Explain the concept of a partial derivative and how it is used in multivariable optimization.](./Round2/10_partial_derivative_multivariable.ipynb) | Describes how partial derivatives extend differentiation to functions of several variables, essential for multivariable optimization.     |
