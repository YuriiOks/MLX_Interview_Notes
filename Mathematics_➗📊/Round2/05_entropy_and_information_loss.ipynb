{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5ï¸âƒ£ Entropy in Information Theory & Its Role in Machine Learning ğŸ“ŠğŸ¤–**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ’¡ Real-Life Analogy: Predicting the Outcome of a Dice Roll ğŸ²**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine youâ€™re **betting on a dice roll**:  \n",
    "- If you roll a **fair die (1â€“6 equally likely)** â†’ **High uncertainty** (more surprise).  \n",
    "- If the die is **loaded (always lands on 6)** â†’ **No uncertainty** (zero surprise).  \n",
    "- The **more unpredictable the outcome**, the **higher the entropy**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Entropy measures uncertainty in a systemâ€”how \"random\" or \"predictable\" information is.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ“Œ What is Entropy in Information Theory?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Entropy quantifies the amount of uncertainty or randomness in a probability distribution.**  \n",
    "âœ… Introduced by **Claude Shannon**, it is fundamental in **data compression, cryptography, and machine learning**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Mathematical Formula (Shannon Entropy):**  \n",
    "For a discrete random variable $X$ with $n$ possible outcomes and probabilities $P(x_i)$:  \n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $H(X)$ = **Entropy (measured in bits)**.  \n",
    "- $P(x_i)$ = **Probability of outcome $x_i$**.  \n",
    "- **Higher entropy** â†’ More unpredictability.  \n",
    "- **Lower entropy** â†’ More certainty.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Key Observations:**  \n",
    "- **If all outcomes are equally likely** â†’ Entropy is **maximum**.  \n",
    "- **If one outcome is certain** â†’ Entropy is **zero**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ“Š Example: Computing Entropy in Coin Toss & Dice Rolls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Scenario | Probability Distribution $ P(x) $ | Entropy $ H(X) $ |  \n",
    "|----------|----------------|------------|  \n",
    "| **Fair Coin Toss** ğŸª™ | $ P(H) = 0.5, P(T) = 0.5 $ | $ -[0.5 \\log_2 0.5 + 0.5 \\log_2 0.5] = 1.0 $ |  \n",
    "| **Loaded Coin (90% Heads)** | $ P(H) = 0.9, P(T) = 0.1 $ | $ -[0.9 \\log_2 0.9 + 0.1 \\log_2 0.1] \\approx 0.47 $ |  \n",
    "| **Fair Die ğŸ²** | $ P(1) = P(2) = ... = P(6) = 1/6 $ | $ H(X) = -\\sum_{i=1}^{6} (1/6) \\log_2 (1/6) \\approx 2.58 $ |  \n",
    "| **Loaded Die (Always 6)** | $ P(6) = 1, P(\\text{others}) = 0 $ | $ H(X) = 0 $ |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Interpretation:**  \n",
    "- **Fair coin/die â†’ High entropy (more uncertainty).**  \n",
    "- **Loaded dice/biased coin â†’ Low entropy (more certainty).**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ”„ Role of Entropy in Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Entropy is used in decision trees, classification models, and information gain calculations.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1ï¸âƒ£ Entropy in Decision Trees (ID3, C4.5) ğŸŒ³**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Goal:** Select the best feature to split data.  \n",
    "- A **good split reduces uncertainty (entropy decreases).**  \n",
    "- We use **Information Gain (IG)** to measure improvement:  \n",
    "\n",
    "$$\n",
    "IG = H(X) - H(X \\mid \\text{feature})\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Example: Choosing the Best Feature**  \n",
    "| Weather | Play Tennis? |  \n",
    "|---------|-------------|  \n",
    "| Sunny   | No          |  \n",
    "| Rainy   | Yes         |  \n",
    "| Cloudy  | Yes         |  \n",
    "\n",
    "- Before splitting â†’ **Entropy is high** (uncertain).  \n",
    "- After splitting â†’ **Entropy is lower** (better classification).  \n",
    "ğŸ“Œ **Entropy helps determine the best decision boundary!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2ï¸âƒ£ Entropy in Probability Models (Cross-Entropy Loss) ğŸ”¢**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Cross-entropy loss** measures how different two probability distributions are.  \n",
    "âœ… Used in **classification tasks (logistic regression, neural networks)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Formula (Cross-Entropy for True Label $ y $ and Prediction $ \\hat{y} $):**  \n",
    "\n",
    "$$\n",
    "H(y, \\hat{y}) = -\\sum y_i \\log(\\hat{y_i})\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Example: Binary Classification (Spam vs. Not Spam ğŸ“§)**  \n",
    "| True Label $ y $ | Predicted $ \\hat{y} $ | Cross-Entropy Loss |  \n",
    "|----------------|----------------|----------------|  \n",
    "| 1 (Spam)     | 0.9            | Low Loss (good prediction) âœ…  |  \n",
    "| 1 (Spam)     | 0.1            | High Loss (bad prediction) âŒ  |  \n",
    "\n",
    "ğŸ“Œ **Goal:** Minimize cross-entropy loss to improve model accuracy!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ› ï¸ Python Code: Computing Entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.00 bits\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Define probabilities (fair coin)\n",
    "p = [0.5, 0.5]\n",
    "\n",
    "# Compute entropy\n",
    "H = entropy(p, base=2)\n",
    "print(f\"Entropy: {H:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Output:**  \n",
    "```\n",
    "Entropy: 1.00 bits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸš€ Applications of Entropy in AI & ML ğŸ¤–**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Decision Trees (ID3, C4.5, CART):** Splitting features based on information gain ğŸŒ³  \n",
    "âœ… **Deep Learning (Cross-Entropy Loss):** Optimizing classification models ğŸ¤–  \n",
    "âœ… **Data Compression (Huffman Coding):** Encoding data efficiently ğŸ”„  \n",
    "âœ… **Cryptography:** Measuring randomness of encryption keys ğŸ”  \n",
    "âœ… **Natural Language Processing (NLP):** Analyzing text predictability ğŸ“œ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ”¥ Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ **Entropy measures uncertainty in a probability distribution.**  \n",
    "2ï¸âƒ£ **Formula:** $H(X) = -\\sum P(x_i) \\log_2 P(x_i)$.  \n",
    "3ï¸âƒ£ **Used in decision trees, classification models, and information gain calculations.**  \n",
    "4ï¸âƒ£ **Lower entropy â†’ More certainty; Higher entropy â†’ More unpredictability.**  \n",
    "5ï¸âƒ£ **Applied in ML for feature selection, loss functions, and NLP.**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
