{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5️⃣ Entropy in Information Theory & Its Role in Machine Learning 📊🤖**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **💡 Real-Life Analogy: Predicting the Outcome of a Dice Roll 🎲**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you’re **betting on a dice roll**:  \n",
    "- If you roll a **fair die (1–6 equally likely)** → **High uncertainty** (more surprise).  \n",
    "- If the die is **loaded (always lands on 6)** → **No uncertainty** (zero surprise).  \n",
    "- The **more unpredictable the outcome**, the **higher the entropy**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Entropy measures uncertainty in a system—how \"random\" or \"predictable\" information is.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **📌 What is Entropy in Information Theory?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Entropy quantifies the amount of uncertainty or randomness in a probability distribution.**  \n",
    "✅ Introduced by **Claude Shannon**, it is fundamental in **data compression, cryptography, and machine learning**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Mathematical Formula (Shannon Entropy):**  \n",
    "For a discrete random variable $X$ with $n$ possible outcomes and probabilities $P(x_i)$:  \n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $H(X)$ = **Entropy (measured in bits)**.  \n",
    "- $P(x_i)$ = **Probability of outcome $x_i$**.  \n",
    "- **Higher entropy** → More unpredictability.  \n",
    "- **Lower entropy** → More certainty.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Key Observations:**  \n",
    "- **If all outcomes are equally likely** → Entropy is **maximum**.  \n",
    "- **If one outcome is certain** → Entropy is **zero**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **📊 Example: Computing Entropy in Coin Toss & Dice Rolls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Scenario | Probability Distribution $ P(x) $ | Entropy $ H(X) $ |  \n",
    "|----------|----------------|------------|  \n",
    "| **Fair Coin Toss** 🪙 | $ P(H) = 0.5, P(T) = 0.5 $ | $ -[0.5 \\log_2 0.5 + 0.5 \\log_2 0.5] = 1.0 $ |  \n",
    "| **Loaded Coin (90% Heads)** | $ P(H) = 0.9, P(T) = 0.1 $ | $ -[0.9 \\log_2 0.9 + 0.1 \\log_2 0.1] \\approx 0.47 $ |  \n",
    "| **Fair Die 🎲** | $ P(1) = P(2) = ... = P(6) = 1/6 $ | $ H(X) = -\\sum_{i=1}^{6} (1/6) \\log_2 (1/6) \\approx 2.58 $ |  \n",
    "| **Loaded Die (Always 6)** | $ P(6) = 1, P(\\text{others}) = 0 $ | $ H(X) = 0 $ |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Interpretation:**  \n",
    "- **Fair coin/die → High entropy (more uncertainty).**  \n",
    "- **Loaded dice/biased coin → Low entropy (more certainty).**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔄 Role of Entropy in Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Entropy is used in decision trees, classification models, and information gain calculations.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1️⃣ Entropy in Decision Trees (ID3, C4.5) 🌳**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Goal:** Select the best feature to split data.  \n",
    "- A **good split reduces uncertainty (entropy decreases).**  \n",
    "- We use **Information Gain (IG)** to measure improvement:  \n",
    "\n",
    "$$\n",
    "IG = H(X) - H(X \\mid \\text{feature})\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Example: Choosing the Best Feature**  \n",
    "| Weather | Play Tennis? |  \n",
    "|---------|-------------|  \n",
    "| Sunny   | No          |  \n",
    "| Rainy   | Yes         |  \n",
    "| Cloudy  | Yes         |  \n",
    "\n",
    "- Before splitting → **Entropy is high** (uncertain).  \n",
    "- After splitting → **Entropy is lower** (better classification).  \n",
    "📌 **Entropy helps determine the best decision boundary!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2️⃣ Entropy in Probability Models (Cross-Entropy Loss) 🔢**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Cross-entropy loss** measures how different two probability distributions are.  \n",
    "✅ Used in **classification tasks (logistic regression, neural networks)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Formula (Cross-Entropy for True Label $ y $ and Prediction $ \\hat{y} $):**  \n",
    "\n",
    "$$\n",
    "H(y, \\hat{y}) = -\\sum y_i \\log(\\hat{y_i})\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Example: Binary Classification (Spam vs. Not Spam 📧)**  \n",
    "| True Label $ y $ | Predicted $ \\hat{y} $ | Cross-Entropy Loss |  \n",
    "|----------------|----------------|----------------|  \n",
    "| 1 (Spam)     | 0.9            | Low Loss (good prediction) ✅  |  \n",
    "| 1 (Spam)     | 0.1            | High Loss (bad prediction) ❌  |  \n",
    "\n",
    "📌 **Goal:** Minimize cross-entropy loss to improve model accuracy!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🛠️ Python Code: Computing Entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.00 bits\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Define probabilities (fair coin)\n",
    "p = [0.5, 0.5]\n",
    "\n",
    "# Compute entropy\n",
    "H = entropy(p, base=2)\n",
    "print(f\"Entropy: {H:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Output:**  \n",
    "```\n",
    "Entropy: 1.00 bits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🚀 Applications of Entropy in AI & ML 🤖**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Decision Trees (ID3, C4.5, CART):** Splitting features based on information gain 🌳  \n",
    "✅ **Deep Learning (Cross-Entropy Loss):** Optimizing classification models 🤖  \n",
    "✅ **Data Compression (Huffman Coding):** Encoding data efficiently 🔄  \n",
    "✅ **Cryptography:** Measuring randomness of encryption keys 🔐  \n",
    "✅ **Natural Language Processing (NLP):** Analyzing text predictability 📜  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔥 Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ **Entropy measures uncertainty in a probability distribution.**  \n",
    "2️⃣ **Formula:** $H(X) = -\\sum P(x_i) \\log_2 P(x_i)$.  \n",
    "3️⃣ **Used in decision trees, classification models, and information gain calculations.**  \n",
    "4️⃣ **Lower entropy → More certainty; Higher entropy → More unpredictability.**  \n",
    "5️⃣ **Applied in ML for feature selection, loss functions, and NLP.**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
