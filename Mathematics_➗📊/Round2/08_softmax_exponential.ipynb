{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8Ô∏è‚É£ Softmax Function: Why Exponentials & Normalization? üìäü§ñ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üí° Real-Life Analogy: Deciding What to Eat at a Buffet üçïüçîüç£**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you‚Äôre at a buffet with **3 dishes**:  \n",
    "- **Pizza (7/10 preference)** üçï  \n",
    "- **Burger (5/10 preference)** üçî  \n",
    "- **Sushi (8/10 preference)** üç£  \n",
    "\n",
    "üìå **How do you assign probabilities to each dish?**  \n",
    "- You could say, ‚ÄúI like pizza 7/10, burger 5/10, and sushi 8/10‚Äù **(raw scores/logits)**.  \n",
    "- But to get **probabilities** (values between **0 and 1** that sum to **1**),  \n",
    "  - **Use exponentials to amplify differences** üî•  \n",
    "  - **Normalize by dividing by the sum** to get a valid probability distribution ‚úÖ  \n",
    "\n",
    "üìå **This is exactly what the Softmax function does!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå What is the Softmax Function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ The **Softmax function** converts a vector of raw scores (**logits**) into a **probability distribution**.  \n",
    "‚úÖ Ensures that outputs:  \n",
    "  - Are **positive**  \n",
    "  - Sum to **1**  (values between **0 and 1** and a valid probability distribution)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Mathematical Formula (Softmax Function):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector of logits $z = [z_1, z_2, \\dots, z_n]$, Softmax is:  \n",
    "$$\n",
    "S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $z_i$ = **Raw score (logit) for class $i$**  \n",
    "- $e^{z_i}$ = **Exponential function** (makes all values positive & amplifies differences)  \n",
    "- $\\sum e^{z_j}$ = **Normalization term** (ensures probabilities sum to 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìä Example: Softmax Calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Given logits:**  \n",
    "$$z = [2, 1, 0]$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Step 1: Compute Exponentials**  \n",
    "$$e^2 = 7.39, \\quad e^1 = 2.72, \\quad e^0 = 1.00$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Step 2: Compute the Sum of Exponentials**  \n",
    "$$7.39 + 2.72 + 1.00 = 11.11$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Step 3: Compute Softmax Probabilities**  \n",
    "$$S(2) = \\frac{7.39}{11.11} = 0.665, \\quad S(1) = \\frac{2.72}{11.11} = 0.245, \\quad S(0) = \\frac{1.00}{11.11} = 0.090$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Final Probability Distribution:**  \n",
    "\n",
    "| Class   | Logit $z$ | $e^z$   | Softmax Probability $S(z)$ |  \n",
    "|---------|-----------|---------|----------------------------|  \n",
    "| Class 1 | 2         | 7.39    | 0.665                      |  \n",
    "| Class 2 | 1         | 2.72    | 0.245                      |  \n",
    "| Class 3 | 0         | 1.00    | 0.090                      |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Interpretation:**  \n",
    "- Class **1 has the highest probability (66.5%)**.  \n",
    "- Class **3 is least likely (9%)**.  \n",
    "- **All probabilities sum to 1** ‚úÖ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîé Why Do We Use Exponentials?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **1Ô∏è‚É£ Ensures All Values Are Positive**  \n",
    "- Some logits may be **negative**  ‚Üí Exponential **makes them positive**.  \n",
    "- Example: If logits = $[-2, 0, 3]$, exponentiation transforms them into **positive values**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **2Ô∏è‚É£ Amplifies Large Differences** üî•  \n",
    "- Small logit differences **become larger** after exponentiation.  \n",
    "- Example: If logits are **[10, 9, 8]**, the raw difference between 10 and 8 is **2**,  \n",
    "  - But after exponentiation:  \n",
    "    - $e^{10} = 22026$  \n",
    "    - $e^{9} = 8103$  \n",
    "    - $e^{8} = 2980$  \n",
    "  - The gap between 10 and 8 **increases significantly**, making class 1 much more confident.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **3Ô∏è‚É£ Mimics a ‚ÄúWinner-Takes-Most‚Äù Effect** üéØ  \n",
    "- If one class has a much higher logit, **Softmax assigns it a very high probability**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîé Why Do We Divide by the Sum of Exponentials?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **1Ô∏è‚É£ Normalization ‚Üí Ensures Probabilities Sum to 1**  \n",
    "- Without division, we would get **unbounded values** (not valid probabilities).  \n",
    "- Example: If logits = [3, 1], exponentials = [20.1, 2.72], but we need:  \n",
    "  $$\\frac{20.1}{20.1 + 2.72} = 0.88, \\quad \\frac{2.72}{20.1 + 2.72} = 0.12$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **2Ô∏è‚É£ Allows Fair Comparison of Different Logit Scales**  \n",
    "- Example: If logits were scaled by **10** (e.g., [30, 10] instead of [3, 1]),  \n",
    "  - The exponentials would explode!  \n",
    "  - Normalization **keeps probabilities meaningful**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå What Are Logits?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Logits are raw scores before Softmax is applied.**  \n",
    "‚úÖ In a neural network:  \n",
    "- The **final layer produces logits** (real numbers, can be negative).  \n",
    "- **Softmax converts them into probabilities** for classification.  \n",
    "‚úÖ **Logits don‚Äôt sum to 1, but Softmax probabilities do!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üõ†Ô∏è Python Code: Softmax Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66524096, 0.24472847, 0.09003057])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define logits\n",
    "logits = np.array([2, 1, 0])\n",
    "\n",
    "# Compute softmax\n",
    "softmax_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "# Replace print with display if needed:\n",
    "display(softmax_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üöÄ Applications of Softmax in AI/ML ü§ñ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Neural Networks (Classification Tasks)**: Converts logits into class probabilities.  \n",
    "‚úÖ **Natural Language Processing (NLP)**: Used in **transformers & LSTMs** for predicting words.  \n",
    "‚úÖ **Reinforcement Learning**: Selects actions based on probability distributions.  \n",
    "‚úÖ **Multi-Class Classification**: Used in **image recognition (e.g., CIFAR-10, MNIST)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üî• Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Softmax converts logits into probabilities using exponentials & normalization.**  \n",
    "2. Exponentials make all values positive & amplify differences.  \n",
    "3. Dividing by the sum ensures probabilities sum to 1.  \n",
    "4. Used in AI/ML for classification tasks, NLP, and reinforcement learning.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
